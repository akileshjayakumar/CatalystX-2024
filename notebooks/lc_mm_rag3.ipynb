{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import logging\n",
    "import glob\n",
    "import os\n",
    "from langchain import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS, VectorStore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    logger.info(f\"Encoding image from path: {image_path}\")\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    logger.info(\"Image encoding complete\")\n",
    "    return encoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing images\n",
    "image_dir = \"/Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images/test1.jpg', '/Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images/test3.jpg', '/Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images/test2.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Get all image files in the directory\n",
    "image_paths = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x10f1cbc80> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x10f1b5b80> model='text-embedding-ada-002' deployment='text-embedding-ada-002' openai_api_version='' openai_api_base=None openai_api_type='' openai_proxy='' embedding_ctx_length=8191 openai_api_key='sk-proj-y8QksusK50Trrm0ZNKTnT3BlbkFJsloxsBlaoS7nGJWT8dva' openai_organization=None allowed_special=set() disallowed_special='all' chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None\n"
     ]
    }
   ],
   "source": [
    "# Initialize FAISS vector store for image embeddings\n",
    "embedding_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_chroma.vectorstores.Chroma object at 0x11fee9ee0>\n",
      "vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x11fee9ee0> docstore=<langchain_core.stores.InMemoryStore object at 0x11f663b90> search_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "print(vectorstore)\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store document entries for vector store\n",
    "documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Encoding image from path: /Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images/test1.jpg\n",
      "INFO:__main__:Image encoding complete\n",
      "INFO:__main__:Sending request to OpenAI API for image: /Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images/test1.jpg\n",
      "INFO:__main__:Request successful\n",
      "INFO:__main__:Encoding image from path: /Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images/test3.jpg\n",
      "INFO:__main__:Image encoding complete\n",
      "INFO:__main__:Sending request to OpenAI API for image: /Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images/test3.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a math page from a workbook focused on finding the areas of rectangles. Here’s a summary of its contents:\n",
      "\n",
      "1. **Math Message**: It asks to find the area of Rectangle A, which is given as 50 square units, with different expressions illustrating this area calculation.\n",
      "\n",
      "2. **Rectangle B**: There are two number sentences written to express its area, calculated as 54 square units.\n",
      "\n",
      "3. **Rectangle C**: The area is provided as 144 square units, with additional calculations involving a variable \\( x \\).\n",
      "\n",
      "4. **Question 4**: Several mathematical expressions describe the areas of rectangles D, E, and F. Each expression is intended to be matched with one of the rectangles, while some matches have already been done.\n",
      "\n",
      "Overall, the image illustrates a lesson on calculating and expressing the areas of rectangles using different methods and representations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Request successful\n",
      "INFO:__main__:Encoding image from path: /Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images/test2.jpg\n",
      "INFO:__main__:Image encoding complete\n",
      "INFO:__main__:Sending request to OpenAI API for image: /Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images/test2.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image contains a worksheet with educational activities. Here's a summary of its contents:\n",
      "\n",
      "1. **Spelling Section**: Instructions for practicing spelling words from a language book, including a testing method using \"look, say, cover.\"\n",
      "\n",
      "2. **Math Section**: A math problem with missing numbers in a basic addition equation. It asks students to determine what the missing numbers could be. The equation sums to \\( \\frac{5}{6} \\).\n",
      "\n",
      "3. **Writing Section**: A prompt encouraging students to continue working on their Rotary Speech.\n",
      "\n",
      "There are also illustrations alongside these sections.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Request successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a worksheet with a question asking to find the area of a triangle. It includes dimensions: a base of 9 cm, a height of 5 cm, and there is a notation \"A =\" indicating where to write the area. The triangle appears to be part of a larger set of math problems, which includes references to other questions and possibly a context about someone named Jordan. The paper looks worn and crumpled.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "import logging\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Function to encode the image\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    logger.info(f\"Encoding image from path: {image_path}\")\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_image = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    logger.info(\"Image encoding complete\")\n",
    "    return encoded_image\n",
    "\n",
    "\n",
    "# Directory containing images\n",
    "image_dir = \"/Users/akileshjayakumar/Documents/catalystX2024/notebooks/mm_rag_images\"\n",
    "\n",
    "# Get all image files in the directory\n",
    "image_paths = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "\n",
    "# List to store summaries\n",
    "cleaned_img_summary = []\n",
    "\n",
    "# Process each image\n",
    "for image_path in image_paths:\n",
    "    base64_image = encode_image(image_path)\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"What’s in this image?\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 300\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Sending request to OpenAI API for image: {image_path}\")\n",
    "    response = requests.post(\n",
    "        \"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        logger.info(\"Request successful\")\n",
    "        summary = response.json().get('choices')[0]['message']['content']\n",
    "        cleaned_img_summary.append(summary)\n",
    "        print(summary)\n",
    "    else:\n",
    "        logger.error(f\"Request failed with status code: {\n",
    "                     response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['651406b2-a7b2-437a-9bdb-2ee34e3b0333',\n",
       " '77387a02-e5f9-4845-83f1-30119a8d9988',\n",
       " '7be0e795-2ccc-438e-a8c8-87b25ec8b751']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "# Create documents from image summaries\n",
    "documents = [Document(page_content=summary, metadata={\n",
    "                      \"doc_id\": str(uuid.uuid4())}) for summary in cleaned_img_summary]\n",
    "\n",
    "# Add documents to the vectorstore\n",
    "vectorstore.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x11fee9ee0> docstore=<langchain_core.stores.InMemoryStore object at 0x11f663b90> search_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "retriever = MultiVectorRetriever(vectorstore=vectorstore, docstore=store, id_key=id_key)\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first={\n",
      "  context: MultiVectorRetriever(vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x11fee9ee0>, docstore=<langchain_core.stores.InMemoryStore object at 0x11f663b90>, search_kwargs={}),\n",
      "  question: RunnablePassthrough()\n",
      "} middle=[ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nAnswer the question based only on the following context: {context}\\nQuestion: {question}\\n'), additional_kwargs={})]), ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x14f13a7b0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x14aa8d9a0>, root_client=<openai.OpenAI object at 0x14ac951c0>, root_async_client=<openai.AsyncOpenAI object at 0x14f1505c0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableSequence\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Option 1: LLM\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "# Option 2: Multi-modal LLM\n",
    "# model = GPT4-V or LLaVA\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\"rertieve all you can\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but there is no context provided for me to retrieve any information. Could you please provide more details or context?\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
