{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "\n",
    "from load_images import load_multimodal_data, load_data_from_directory\n",
    "from load_env import set_environment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the page configuration\n",
    "st.set_page_config(layout=\"wide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize settings\n",
    "def initialize_settings():\n",
    "    Settings.embed_model = NVIDIAEmbedding(\n",
    "        model=\"nvidia/nv-embedqa-e5-v5\", truncate=\"END\")\n",
    "    Settings.llm = NVIDIA(model=\"meta/llama-3.1-70b-instruct\")\n",
    "    Settings.text_splitter = SentenceSplitter(chunk_size=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index from documents\n",
    "def create_index(documents):\n",
    "    vector_store = MilvusVectorStore(\n",
    "        host=\"127.0.0.1\",\n",
    "        port=19530,\n",
    "        dim=1024\n",
    "    )\n",
    "    # vector_store = MilvusVectorStore(uri=\"./milvus_demo.db\", dim=1024, overwrite=True) #For CPU only vector store\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    return VectorStoreIndex.from_documents(documents, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the Streamlit app\n",
    "def main():\n",
    "    set_environment_variables()\n",
    "    initialize_settings()\n",
    "\n",
    "    col1, col2 = st.columns([1, 2])\n",
    "\n",
    "    with col1:\n",
    "        st.title(\"Multimodal RAG\")\n",
    "\n",
    "        input_method = st.radio(\"Choose input method:\",\n",
    "                                (\"Upload Files\", \"Enter Directory Path\"))\n",
    "\n",
    "        if input_method == \"Upload Files\":\n",
    "            uploaded_files = st.file_uploader(\n",
    "                \"Drag and drop files here\", accept_multiple_files=True)\n",
    "            if uploaded_files and st.button(\"Process Files\"):\n",
    "                with st.spinner(\"Processing files...\"):\n",
    "                    documents = load_multimodal_data(uploaded_files)\n",
    "                    st.session_state['index'] = create_index(documents)\n",
    "                    st.session_state['history'] = []\n",
    "                    st.success(\"Files processed and index created!\")\n",
    "        else:\n",
    "            directory_path = st.text_input(\"Enter directory path:\")\n",
    "            if directory_path and st.button(\"Process Directory\"):\n",
    "                if os.path.isdir(directory_path):\n",
    "                    with st.spinner(\"Processing directory...\"):\n",
    "                        documents = load_data_from_directory(directory_path)\n",
    "                        st.session_state['index'] = create_index(documents)\n",
    "                        st.session_state['history'] = []\n",
    "                        st.success(\"Directory processed and index created!\")\n",
    "                else:\n",
    "                    st.error(\"Invalid directory path. Please enter a valid path.\")\n",
    "\n",
    "    with col2:\n",
    "        if 'index' in st.session_state:\n",
    "            st.title(\"Chat\")\n",
    "            if 'history' not in st.session_state:\n",
    "                st.session_state['history'] = []\n",
    "\n",
    "            query_engine = st.session_state['index'].as_query_engine(\n",
    "                similarity_top_k=20, streaming=True)\n",
    "\n",
    "            user_input = st.chat_input(\"Enter your query:\")\n",
    "\n",
    "            # Display chat messages\n",
    "            chat_container = st.container()\n",
    "            with chat_container:\n",
    "                for message in st.session_state['history']:\n",
    "                    with st.chat_message(message[\"role\"]):\n",
    "                        st.markdown(message[\"content\"])\n",
    "\n",
    "            if user_input:\n",
    "                with st.chat_message(\"user\"):\n",
    "                    st.markdown(user_input)\n",
    "                st.session_state['history'].append(\n",
    "                    {\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    message_placeholder = st.empty()\n",
    "                    full_response = \"\"\n",
    "                    response = query_engine.query(user_input)\n",
    "                    for token in response.response_gen:\n",
    "                        full_response += token\n",
    "                        message_placeholder.markdown(full_response + \"â–Œ\")\n",
    "                    message_placeholder.markdown(full_response)\n",
    "                st.session_state['history'].append(\n",
    "                    {\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "            # Add a clear button\n",
    "            if st.button(\"Clear Chat\"):\n",
    "                st.session_state['history'] = []\n",
    "                st.rerun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run /Users/akileshjayakumar/Documents/catalystX2024/.venv/lib/python3.12/site-packages/ipykernel_launcher.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta/llama-3.1-405b-instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hi\"}],\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update in 2021, the President of the United States is Joe Biden. He took office on January 20, 2021. Please note that my information may not be up to date, and you should verify this with a more current source for the most accurate information."
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "client = ChatNVIDIA(\n",
    "    model=\"meta/llama-3.1-405b-instruct\",\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"),\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "for chunk in client.stream([{\"role\": \"user\", \"content\": \"who is the president of the united states?\"}]):\n",
    "  print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
